{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "from transformers import BertTokenizer, DistilBertModel, VisualBertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast, os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "detector = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "batch_size = 4\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image_list):\n",
    "    feature_extractor = nn.Sequential(*list(detector.backbone.children())[:-2])\n",
    "    visual_embeddings = []\n",
    "    for image in image_list:\n",
    "        visual_embedding = feature_extractor(image)\n",
    "        visual_embeddings.append(visual_embedding)\n",
    "    visual_embeddings = torch.stack(visual_embeddings)\n",
    "    return visual_embeddings\n",
    "\n",
    "def map_values(ratings, tags):\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'concrete':\n",
    "            ratings[i] = 0\n",
    "        elif tag == 'middle':\n",
    "            ratings[i] = 1\n",
    "        else:\n",
    "            ratings[i] = 2\n",
    "    return ratings\n",
    "\n",
    "class Multimodal_Dataset(Dataset):\n",
    "    def __init__(self, words_file, image_file, tokenizer, regression=False):\n",
    "        self.words_file = words_file\n",
    "        self.images = image_file\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.data = pd.read_csv(words_file)\n",
    "        self.words = self.data['word'].to_list()\n",
    "        self.encodings = self.tokenizer([word for word in self.words], add_special_tokens=True, padding='max_length', max_length = 12 ,return_tensors='pt')\n",
    "\n",
    "        self.photos = self.data['photos'].apply(ast.literal_eval)\n",
    "        self.labels = self.data['tag'].to_list()\n",
    "        if regression == True:\n",
    "            self.ratings = self.data['rating'].to_list()\n",
    "        else:\n",
    "            ratings = self.data['rating'].to_list()\n",
    "            self.ratings = map_values(ratings, self.labels)\n",
    "\n",
    "        transform_list = [\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.Resize((32, 168)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.photos[idx]\n",
    "        imgs = []\n",
    "        for image in images:\n",
    "          img_path = os.path.join('images/', image)\n",
    "          img = Image.open(img_path)\n",
    "          img = self.transform(img)\n",
    "          imgs.append(img)\n",
    "\n",
    "        # #padding list of images, without this the dataloader results in errors\n",
    "        while len(imgs) < 12:\n",
    "            imgs.append(torch.zeros_like(imgs[0]))\n",
    "\n",
    "        embeddings = get_features(imgs)\n",
    "        #imgs = torch.stack(imgs)\n",
    "        #print(f\"Word: {self.words[idx]}, Number of images: {len(images)}\")\n",
    "\n",
    "        item = {'word': self.words[idx], 'input_ids': self.encodings['input_ids'][idx], 'attn_mask': self.encodings['attention_mask'][idx], 'token_type_ids': self.encodings['token_type_ids'],'visual_embeddings': embeddings, 'rating': self.ratings[idx],'label': self.labels[idx]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'rack',\n",
       " 'input_ids': tensor([  101, 14513,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attn_mask': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'visual_embeddings': tensor([[[[-0.3647, -0.3412, -0.3333,  ..., -0.2078, -0.1843, -0.2000],\n",
       "           [-0.3255, -0.3098, -0.3020,  ..., -0.1373, -0.1216, -0.1529],\n",
       "           [-0.2941, -0.2784, -0.2549,  ..., -0.1294, -0.0510, -0.0431],\n",
       "           ...,\n",
       "           [-0.7412, -0.7333, -0.7098,  ..., -0.5059, -0.5059, -0.5373],\n",
       "           [-0.7098, -0.7020, -0.6706,  ..., -0.5294, -0.4745, -0.4667],\n",
       "           [-0.6235, -0.6549, -0.6549,  ..., -0.5373, -0.5137, -0.5843]]],\n",
       " \n",
       " \n",
       "         [[[-0.8275, -0.8588, -0.9059,  ..., -0.4431, -0.3961, -0.4353],\n",
       "           [-0.7569, -0.7961, -0.8353,  ..., -0.1451, -0.3020, -0.4196],\n",
       "           [-0.5137, -0.6000, -0.6549,  ..., -0.0667, -0.2471, -0.2784],\n",
       "           ...,\n",
       "           [-0.3647, -0.3255, -0.3647,  ...,  0.4118,  0.3961,  0.1373],\n",
       "           [-0.5137, -0.5137, -0.5529,  ..., -0.1608, -0.0824, -0.0902],\n",
       "           [-0.2784, -0.3961, -0.3961,  ..., -0.5294, -0.5765, -0.6000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4510,  0.4510,  0.4588,  ..., -0.6941, -0.6549, -0.6549],\n",
       "           [ 0.4824,  0.4824,  0.4902,  ..., -0.7412, -0.6784, -0.6314],\n",
       "           [ 0.5294,  0.5294,  0.5294,  ..., -0.7569, -0.7176, -0.6471],\n",
       "           ...,\n",
       "           [-0.1059, -0.1137, -0.1451,  ..., -0.0510, -0.0824, -0.0667],\n",
       "           [-0.0745, -0.0745, -0.1294,  ..., -0.3882, -0.3569, -0.2157],\n",
       "           [-0.2000, -0.2000, -0.2471,  ..., -0.6627, -0.6627, -0.6235]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.4118, -0.3961, -0.3882,  ..., -0.5608, -0.4745, -0.5059],\n",
       "           [-0.3804, -0.3647, -0.3569,  ..., -0.4824, -0.4118, -0.4431],\n",
       "           [-0.3412, -0.3255, -0.3176,  ..., -0.3882, -0.3490, -0.3804],\n",
       "           ...,\n",
       "           [-0.3333, -0.3255, -0.3255,  ...,  0.2314,  0.2000,  0.1373],\n",
       "           [-0.3804, -0.3725, -0.3647,  ..., -0.0353, -0.0510, -0.0275],\n",
       "           [-0.4275, -0.4118, -0.4039,  ...,  0.6157,  0.6000,  0.5529]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1686,  0.2392,  0.3176,  ..., -0.8118, -0.8118, -0.7882],\n",
       "           [ 0.5608,  0.6314,  0.6941,  ..., -0.8902, -0.8824, -0.8824],\n",
       "           [ 0.5608,  0.6392,  0.7020,  ..., -0.8745, -0.8667, -0.8667],\n",
       "           ...,\n",
       "           [-0.7412, -0.7333, -0.7176,  ..., -0.3020, -0.3882, -0.4431],\n",
       "           [-0.7569, -0.7490, -0.7412,  ..., -0.4667, -0.4902, -0.4980],\n",
       "           [-0.6784, -0.7255, -0.7412,  ..., -0.5059, -0.5373, -0.5922]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]]),\n",
       " 'rating': 0,\n",
       " 'label': 'concrete'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Multimodal_Dataset('merged_data.csv', 'images', tokenizer=tokenizer)\n",
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, batch in enumerate(dataloader):\n",
    " #   print(batch['visual_embeddings'].size())\n",
    "  #  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Textual BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXTUAL_BERT(nn.Module):\n",
    "  def __init__(self, num_of_labels):\n",
    "      super(TEXTUAL_BERT, self).__init__()\n",
    "      self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "      self.classifier = nn.Linear(self.bert.config.hidden_size, num_of_labels)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "      outputs = self.bert(input_ids, attention_mask)\n",
    "      predictions = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "      return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.1138059186935425\n"
     ]
    }
   ],
   "source": [
    "model = TEXTUAL_BERT(3).to(device)\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "gold_labels = []\n",
    "misclassifications = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).long().to(device)\n",
    "        gold_label = batch['rating'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "        \n",
    "        gold_labels.extend(gold_label.cpu().numpy())\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        \n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "        for i in range(len(predicted_labels)):\n",
    "            if predicted_labels[i] != gold_label[i]:\n",
    "                misclassification = f\"{batch['word'][i]} predicted as {predicted_labels[i]} instead of {gold_label[i]}\"\n",
    "                misclassifications.append(misclassification)\n",
    "    \n",
    "average_loss = total_loss / len(dataloader)\n",
    "print(f'Average Loss: {average_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.93      0.48       100\n",
      "           1       0.30      0.03      0.05       100\n",
      "           2       0.00      0.00      0.00       100\n",
      "\n",
      "    accuracy                           0.32       300\n",
      "   macro avg       0.21      0.32      0.18       300\n",
      "weighted avg       0.21      0.32      0.18       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(gold_labels, predictions)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 loss: 1.1049219648043314\r"
     ]
    }
   ],
   "source": [
    "model = TEXTUAL_BERT(3).to(device)\n",
    "epochs = 30\n",
    "model.train()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).long().to(device)\n",
    "        gold_label = batch['rating'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch:\",epoch, \"loss:\", total_loss/(i+1), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.7636477500200272\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "predictions = []\n",
    "gold_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).long().to(device)\n",
    "        gold_label = batch['rating'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "\n",
    "        gold_labels.extend(gold_label.cpu().numpy())\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label.unsqueeze(1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        #print(gold_label.size(), predicted_labels.size())\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      1.00      0.54        22\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.37        60\n",
      "   macro avg       0.12      0.33      0.18        60\n",
      "weighted avg       0.13      0.37      0.20        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(gold_labels, predictions)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_data = Multimodal_Dataset('merged_data.csv', 'images', tokenizer=tokenizer, regression=True)\n",
    "regression_dataloader = DataLoader(regression_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 11.16761583803292\n"
     ]
    }
   ],
   "source": [
    "model = TEXTUAL_BERT(1).to(device)\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "gold_labels = []\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in regression_dataloader:\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).long().to(device)\n",
    "        gold_label = batch['rating'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attn_masks)\n",
    "        \n",
    "        gold_labels.extend(gold_label.cpu().numpy())\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label.unsqueeze(1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        #print(gold_label.size(), predicted_labels.size())\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "average_loss = total_loss / len(regression_dataloader)\n",
    "print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: nan\n",
      "P-value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusfilvi@GU.GU.SE/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation_coefficient, p_value = pearsonr(gold_labels, predictions)\n",
    "#The model sometimes predicts only one value and as a result the pearson correlation cannot be computed\n",
    "print(f\"Pearson Correlation Coefficient: {correlation_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.2834\n",
      "MAE: 3.0588\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(gold_labels, predictions, squared=False)\n",
    "mae = mean_absolute_error(gold_labels, predictions)\n",
    "\n",
    "print(f\"MSE: {mse.item():.4f}\")\n",
    "print(f\"MAE: {mae.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISUAL_BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VISUAL_BERT, self).__init__()\n",
    "        self.visual_bert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "        #self.classifier = nn.Linear(self.visual_bert.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids, visual_embeddings):\n",
    "        \n",
    "        visual_token_type_ids = torch.ones(visual_embeddings.shape[:-1], dtype=torch.long).to(device)\n",
    "        visual_attention_mask = torch.ones(visual_embeddings.shape[:-1], dtype=torch.float).to(device)\n",
    "        print('visual attn mask', visual_attention_mask.squeeze(2).size())\n",
    "        #print('attn mask', attn_masks.size())\n",
    "        outputs = self.visual_bert(input_ids=input_ids, attention_mask=attn_masks, token_type_ids=token_type_ids, visual_embeds=visual_embeddings, visual_attention_mask=visual_attention_mask.squeeze(2), visual_token_type_ids=visual_token_type_ids)\n",
    "        #predictions = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 1, 32, 168])\n",
      "visual attn mask torch.Size([4, 12, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1536x168 and 2048x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m gold_label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(gold_label.size())\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_and_vision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     24\u001b[0m gold_labels\u001b[38;5;241m.\u001b[39mextend(gold_label\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m, in \u001b[0;36mVISUAL_BERT.forward\u001b[0;34m(self, input_ids, attn_masks, token_type_ids, visual_embeddings)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisual attn mask\u001b[39m\u001b[38;5;124m'\u001b[39m, visual_attention_mask\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#print('attn mask', attn_masks.size())\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#predictions = self.classifier(outputs.last_hidden_state[:, 0, :])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:805\u001b[0m, in \u001b[0;36mVisualBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    803\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 805\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_text_alignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_text_alignment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbypass_transformer \u001b[38;5;129;01mand\u001b[39;00m visual_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     text_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/transformers/models/visual_bert/modeling_visual_bert.py:140\u001b[0m, in \u001b[0;36mVisualBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, visual_embeds, visual_token_type_ids, image_text_alignment)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visual_token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     visual_token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    137\u001b[0m         visual_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    138\u001b[0m     )\n\u001b[0;32m--> 140\u001b[0m visual_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m visual_token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_token_type_embeddings(visual_token_type_ids)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_text_alignment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# image_text_alignment = Batch x image_length x alignment_number.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Each element denotes the position of the word corresponding to the image feature. -1 is the padding value.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/AICS/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1536x168 and 2048x768)"
     ]
    }
   ],
   "source": [
    "language_and_vision_model = VISUAL_BERT().to(device)\n",
    "\n",
    "language_and_vision_model.eval()\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "gold_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids = torch.Tensor(batch['input_ids']).long().to(device)\n",
    "        #print(input_ids.size())\n",
    "        attn_masks = torch.Tensor(batch['attn_mask']).long().to(device)\n",
    "        token_type_ids = torch.mean(batch['token_type_ids'].float(), dim=1).to(device)\n",
    "        token_type_ids = token_type_ids.long()\n",
    "        #print(token_type_ids.size())\n",
    "        visual_embeddings = batch['visual_embeddings'].to(device)\n",
    "        print(visual_embeddings.size())\n",
    "        gold_label = batch['rating'].to(device)\n",
    "        #print(gold_label.size())\n",
    "    \n",
    "        outputs = language_and_vision_model(input_ids, attn_masks.unsqueeze(2), token_type_ids, visual_embeddings)\n",
    "            \n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        gold_labels.extend(gold_label.cpu().numpy())\n",
    "\n",
    "        loss = loss_fn(outputs, gold_label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(dataloader)\n",
    "print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(gold_labels, predictions, squared=False)\n",
    "mae = mean_absolute_error(gold_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse.item():.4f}\")\n",
    "print(f\"MAE: {mae.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AICS",
   "language": "python",
   "name": "aics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
