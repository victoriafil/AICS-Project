{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "u5kRNkQbL1xh",
      "metadata": {
        "id": "u5kRNkQbL1xh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "import os\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Fhrs7iBHMBM1",
      "metadata": {
        "id": "Fhrs7iBHMBM1"
      },
      "outputs": [],
      "source": [
        "def check_pos(words):\n",
        "    nouns = []\n",
        "    for doc in nlp.pipe(map(str, words)):\n",
        "        for token in doc:\n",
        "            if token.tag_ == \"NN\":\n",
        "                nouns.append(token.text)\n",
        "    return nouns\n",
        "\n",
        "def get_nouns(xl_file):\n",
        "    df = pd.read_excel(xl_file)\n",
        "    words = df['Word'].to_list()\n",
        "    processed_words = check_pos(words)\n",
        "    ratings = df['Conc.M'].to_list()\n",
        "    dictionary_of_ratings = {\"concrete\" : [], \"middle\" : [], \"abstract\" : []}\n",
        "    for word, rating in zip(words, ratings):\n",
        "        if word in processed_words:\n",
        "            if rating <= 2:\n",
        "              dictionary_of_ratings[\"abstract\"].append((word, rating))\n",
        "            elif rating > 2 and rating < 4:\n",
        "              dictionary_of_ratings[\"middle\"].append((word, rating))\n",
        "            elif rating > 4:\n",
        "              dictionary_of_ratings[\"concrete\"].append((word, rating))\n",
        "    return dictionary_of_ratings\n",
        "\n",
        "dictionary_of_ratings = get_nouns(\"brysbaert_dataset.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9sz0LDy3MQOA",
      "metadata": {
        "id": "9sz0LDy3MQOA"
      },
      "outputs": [],
      "source": [
        "def create_data(wordlist, tag):\n",
        "    dictionaries = []\n",
        "    for i in range(len(wordlist)):\n",
        "        data = {\"word\" : wordlist[i][0], \"rating\" : wordlist[i][1], \"tag\" : tag}\n",
        "        dictionaries.append(data)\n",
        "    return dictionaries\n",
        "\n",
        "def get_random_nouns(dictionary):\n",
        "    new_dictionaries = []\n",
        "    concrete = dictionary[\"concrete\"]\n",
        "    middle = dictionary[\"middle\"]\n",
        "    abstract = dictionary[\"abstract\"]\n",
        "\n",
        "    conc = create_data(random.sample(concrete, 100), \"concrete\")\n",
        "    mid = create_data(random.sample(middle, 100), \"middle\")\n",
        "    abs = create_data(random.sample(abstract, 100), \"abstract\")\n",
        "\n",
        "    new_dictionaries.append(conc)\n",
        "    new_dictionaries.append(mid)\n",
        "    new_dictionaries.append(abs)\n",
        "\n",
        "    return new_dictionaries\n",
        "\n",
        "data = get_random_nouns(dictionary_of_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "xCeha6CTMd5z",
      "metadata": {
        "id": "xCeha6CTMd5z"
      },
      "outputs": [],
      "source": [
        "def create_data_table(tag, dictionary_list):\n",
        "    column_names=[\"word\", \"rating\", \"tag\"]\n",
        "\n",
        "    df = pd.DataFrame(dictionary_list, columns=column_names)\n",
        "    filepath = tag + \"_dataset.csv\"\n",
        "    file = df.to_csv(filepath, index=False)\n",
        "    read_file = pd.read_csv(filepath)\n",
        "\n",
        "#create_data_table(\"concrete\", data[0])\n",
        "#create_data_table(\"middle\", data[1])\n",
        "#create_data_table(\"abstract\", data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03525d02-cbb5-45da-9d96-deb2e76bb219",
      "metadata": {
        "id": "03525d02-cbb5-45da-9d96-deb2e76bb219"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gusfilvi@GU.GU.SE/.conda/envs/AICS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "1e57b03e",
      "metadata": {
        "id": "1e57b03e"
      },
      "outputs": [],
      "source": [
        "def get_concrete_words(file):\n",
        "  file_name = os.path.basename(file)\n",
        "  if 'xlsx'in file_name:\n",
        "    df = pd.read_excel(file)\n",
        "    words = df['Word'].to_list()\n",
        "    ratings = df['Conc.M'].to_list()\n",
        "    concrete_words = {'word' : []}\n",
        "    for word, rating in zip(words, ratings):\n",
        "        if rating > 4:\n",
        "            concrete_words['word'].append(word)\n",
        "    return concrete_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "1a0ceae2",
      "metadata": {
        "id": "1a0ceae2"
      },
      "outputs": [],
      "source": [
        "concrete_words = get_concrete_words('brysbaert_dataset.xlsx')\n",
        "#df = pd.DataFrame(concrete_words)\n",
        "#df.to_csv('all_concrete_words.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e3226fa2",
      "metadata": {
        "id": "e3226fa2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('all_concrete_words.csv')\n",
        "text = df['word']\n",
        "encoder = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
        "vectors = encoder.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b71506c6",
      "metadata": {
        "id": "b71506c6"
      },
      "outputs": [],
      "source": [
        "vector_dimension = vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(vector_dimension)\n",
        "faiss.normalize_L2(vectors)\n",
        "index.add(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "1043a20d",
      "metadata": {
        "id": "1043a20d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "search_text = 'alzheimer'\n",
        "search_vector = encoder.encode(search_text)\n",
        "_vector = np.array([search_vector])\n",
        "faiss.normalize_L2(_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "8cfbc33a",
      "metadata": {
        "id": "8cfbc33a"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "distances, ann = index.search(_vector, k=k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "a4e5708c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4e5708c",
        "outputId": "0cc9621d-80f2-41b3-c165-004f0ebaf720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['brain', 'grandmother', 'grandma']\n"
          ]
        }
      ],
      "source": [
        "results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
        "merge = pd.merge(results, df, left_on='ann', right_index=True)\n",
        "merge\n",
        "words = text\n",
        "neighbors = [words[idx] for idx in ann[0]]\n",
        "print(neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ebdd14c4",
      "metadata": {
        "id": "ebdd14c4"
      },
      "outputs": [],
      "source": [
        "def get_nearest_neighbors(abstract_file):\n",
        "    all_concrete_words = pd.read_csv('all_concrete_words.csv')\n",
        "    all_concrete_words = all_concrete_words['word']\n",
        "    encoder = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
        "    vectors = encoder.encode(all_concrete_words)\n",
        "\n",
        "    vector_dimension = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(vector_dimension)\n",
        "    faiss.normalize_L2(vectors)\n",
        "    index.add(vectors)\n",
        "\n",
        "    list_of_dicts = []\n",
        "    df = pd.read_csv(abstract_file)\n",
        "    words = df['word'].to_list()\n",
        "    for word in words:\n",
        "        search_vector = encoder.encode(word)\n",
        "        _vector = np.array([search_vector])\n",
        "        faiss.normalize_L2(_vector)\n",
        "        distances, ann = index.search(_vector, k=3)\n",
        "        neighbors = [all_concrete_words[idx] for idx in ann[0]]\n",
        "        dictionary = {'word' : word, \"neighbours\" : neighbors}\n",
        "        list_of_dicts.append(dictionary)\n",
        "    return list_of_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "d0c1029c",
      "metadata": {
        "id": "d0c1029c"
      },
      "outputs": [],
      "source": [
        "middle_neighbors = get_nearest_neighbors('middle_dataset.csv')\n",
        "abstract_neighbors = get_nearest_neighbors('abstract_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "1bd7bb4a",
      "metadata": {
        "id": "1bd7bb4a"
      },
      "outputs": [],
      "source": [
        "def update_table(existing_file, dictionary):\n",
        "    existing_data = pd.read_csv(existing_file)\n",
        "    new_data = pd.DataFrame(dictionary)\n",
        "\n",
        "    merged_file = pd.merge(existing_data, new_data, on='word', how='outer')\n",
        "    merged_file.to_csv(existing_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "84ec60d5",
      "metadata": {
        "id": "84ec60d5"
      },
      "outputs": [],
      "source": [
        "#update_table('middle_dataset.csv', middle_neighbors)\n",
        "#update_table('abstract_dataset.csv', abstract_neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dr4rVKmVLTYu",
      "metadata": {
        "id": "dr4rVKmVLTYu"
      },
      "outputs": [],
      "source": [
        "#!pip install flickrapi\n",
        "import flickrapi\n",
        "import requests "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "MWXEIrS4QGsV",
      "metadata": {
        "id": "MWXEIrS4QGsV"
      },
      "outputs": [],
      "source": [
        "flickr_keys = {'key' : u'c9221c0662473c4749ebbf3ee984ca7b',\n",
        "               'secret' : u'd2ee04a8ab2d9f4e'}\n",
        "\n",
        "key, secret = flickr_keys.values()\n",
        "\n",
        "flickr = flickrapi.FlickrAPI(key, secret, format = 'parsed-json')\n",
        "#gets images for concrete words\n",
        "def get_photos(datafile):\n",
        "    df = pd.read_csv(datafile)\n",
        "    list_of_urls = []\n",
        "    words = df['word'].to_list()\n",
        "    for word in words:\n",
        "        objects = flickr.photos.search(\n",
        "            text = word, # Search term\n",
        "            per_page=12,\n",
        "            extras = 'url_c', # Number of results per page\n",
        "            privacy_filter=1, #public photos\n",
        "            safe_search=1, # is safe\n",
        "            sort = 'relevance'\n",
        "            )\n",
        "        urls = []\n",
        "        for object in objects['photos']['photo']:\n",
        "            url = object.get('url_c')\n",
        "            if url is not None:\n",
        "                urls.append(url)\n",
        "        list_of_urls.append(urls)\n",
        "    df['photos'] = list_of_urls\n",
        "    df.to_csv(datafile, index=False)\n",
        "    return list_of_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9wzxCxMSQPOG",
      "metadata": {
        "id": "9wzxCxMSQPOG"
      },
      "outputs": [],
      "source": [
        "urls = get_photos('concrete_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "W-ViT3soQmv_",
      "metadata": {
        "id": "W-ViT3soQmv_"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "def download_image(datafile, directory):\n",
        "    if not os.path.exists(directory):\n",
        "       os.makedirs(directory)\n",
        "    df = pd.read_csv(datafile)\n",
        "    df['photos'] = df['photos'].apply(ast.literal_eval)\n",
        "    urls = df['photos'].to_list()\n",
        "    filenames = []\n",
        "    for url_list in urls:\n",
        "        image_names = []\n",
        "        for url in url_list:\n",
        "            filename = os.path.join(directory, url.split('/')[-1])\n",
        "            response = requests.get(url)\n",
        "            image_names.append(filename.split('/')[-1])\n",
        "            with open(filename, \"wb\") as outputfile:\n",
        "                outputfile.write(response.content)\n",
        "        filenames.append(image_names)\n",
        "    df['photos'] = filenames\n",
        "    df.to_csv(datafile, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fp9iu19pQpKr",
      "metadata": {
        "id": "fp9iu19pQpKr"
      },
      "outputs": [],
      "source": [
        "download_image('concrete_dataset.csv', \"images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dYW1eXa3VdRr",
      "metadata": {
        "id": "dYW1eXa3VdRr"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "#gets images for abstract and middle words\n",
        "def get_photos(datafile):\n",
        "    df = pd.read_csv(datafile)\n",
        "    list_of_urls = []\n",
        "    neighbor_words = df['neighbours'].apply(ast.literal_eval)\n",
        "    for neighbor_list in neighbor_words:\n",
        "      neighbor_urls = []\n",
        "      for neighbor in neighbor_list:\n",
        "        objects = flickr.photos.search(\n",
        "            text = neighbor, # Search term\n",
        "            per_page=4,\n",
        "            extras = 'url_c', # Number of results per page\n",
        "            privacy_filter=1, #public photos\n",
        "            safe_search=1, # is safe\n",
        "            sort = 'relevance'\n",
        "            )\n",
        "        for object in objects['photos']['photo']:\n",
        "            url = object.get('url_c')\n",
        "            if url is not None:\n",
        "                neighbor_urls.append(url)\n",
        "      list_of_urls.append(neighbor_urls)\n",
        "    df['photos'] = list_of_urls\n",
        "    df.to_csv(datafile, index=False)\n",
        "    return list_of_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c6732b07",
      "metadata": {},
      "outputs": [],
      "source": [
        "middle_photos = get_photos('middle_dataset.csv')\n",
        "abstract_photos = get_photos('abstract_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f1fda8a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "download_image('middle_dataset.csv', \"images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd731eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "download_image('abstract_dataset.csv', \"images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11b63e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "conc = pd.read_csv('concrete_dataset.csv')\n",
        "mid = pd.read_csv('middle_dataset.csv')\n",
        "abs = pd.read_csv('abstract_dataset.csv')\n",
        "\n",
        "merged_data = pd.concat([conc, mid, abs], ignore_index=True)\n",
        "merged_data.to_csv('merged_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AICS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
