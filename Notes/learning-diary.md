# Notes/Learning Diary

**18-22/11/2023:** brainstorming project ideas.

**24-29/11/2023:** discussing ideas with Simon, writing down project proposal, and posting it on the Canvas discussion.

**30/11/2023:** presenting and discussing project proposal idea in class. 

**7/12/2023:** first meeting with Elham to talk about the project idea and how to proceed. Talked about methodology for acquiring images for the nouns. Agreed on extracting 100 nouns for each category (concrete, middle, abstract) and 12 images for each noun. This way we would have a balanced dataset for the experiments.

**9-15/12/2023:** data collection for the project. Researched ways to acquire semantic neighbours for the middle and abstract classes and decided on using sentence transformers to encode the words in pair with the Faiss library to search for the approximate neighbours.

**14/12/2023:** second meeting with Elham to discuss the data collection process and the models to be tested. Looked at ClipBERT for the language and vision task.

**18/12/2023:** presenting work and dataset collection procedure in class with slides. Suggestions about which vision and language model to use.

**27-31/12/2023:** decided on which BERT models to use, created dataset class in order to process the words and images. Created a textual BERT model architecture with a classification layer to make predictions about concrete-middle-abstract labels (0-1-2). Evaluated the model on the dataset collected both against the ratings as scalar values for a regression task and as disctinct class labels for a classification task, and noted down the results. The model was randomly predicting one or two classes every time. The regression task was abandoned and more focus was put in training the model on the dataset in order to test if its behavior would change. Experimented with different batch sizes for the training process, learning rates, a bigger dataset, and batch sizes, but its behavior did not change.

**3-12/01/2024:** started implementing the vision and language model architecture using VisualBERT. Researched ways in order to acquire the visual embeddings needed. Initial thought was to use a cnn model in order to acquire the embeddings. Elham suggested using FasterRCNN in order to conduct a phrase to region alignment experiment as well. Started implementing a get_features function that resulted in many shape and size errors since it was not giving back the input that Visual Bert was expecting. Used `nn.Sequential(*list(detector.backbone.children())[:-2])` in order to acquire the visual embeddings. After solving dimension and shape errors, it turned out that this method was not working well with VisualBERT because it resulted in matrix multiplication problems. Started trying to extract features and region proposals based on these features trying to replicate the methods in the hugging face notebook suggested on VisualBERT's page: [colab_notebook](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing).

**12-17/1/2024:** Continued trying to make FasterRCNN work and extract proposals using roi heads. However, I did not manage to get them as I was getting shape and value errors. Started reading source code for VisualBERT and FasterRCNN in order to try and pinpoint the mistake and try to make things work. Messaged Elham, Simon, and Nikolai, who helped me solve the errors by suggesting using a different model to acquire visual embeddings. Started writing the course paper, by writing the introduction, background literature, and the experiments and results until that time.

**17-21/01/2024:** started using ResNet50 in order to acquire the visual embeddings. This worked very well with the nn.Sequential() method and the features generated from there worked well with VisualBERT. Tested VisualBERT on the dataset and noted down the results, its behavior was the same as Textual BERT. Trained and then tested the model again on the dataset collected and observed an increase to its performance. However, after training and evaluating several times, it was noticed that the highest scoring class changes after each training. Talked with NIkolai and speculated on reasons why this might be happening: the input to the models is not in the correct format (the format they were trained on), so they cannot perform well when evaluating, the dataset is too small, the models were not pre-trained on a similar task. Tried to correct the input format: instead of `batch_size x seq_length` it was changed to `batch_size x 1 x seq_length` in order to ensure that the items are separated from each other in each batch. However, this resulted to value errors when passing it to the model which was solved by squeezing the b dimension. This was a little confusing and the inputs were left as in the beginning as there was very little time to continue testings. Finished writing the course paper.